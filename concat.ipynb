{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Package and set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from datetime import date\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home and time\n",
    "home = os.path.expanduser(\"~\")\n",
    "todaystr = date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# set up concat directories\n",
    "targetFolder = os.path.join(home, 'HP Inc','GPSTW SOP - 2021 日新','Project team','Upload folder ( for buyer update )')\n",
    "FD_folder = os.path.join(targetFolder, \"FD_today\")\n",
    "shortage_folder = os.path.join(targetFolder ,\"shortage_today\")\n",
    "PNbasedDetail_folder = os.path.join(targetFolder ,\"PNbasedDetail_today\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and clean FD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat FD\n",
    "FD_files = []\n",
    "for FD_file in glob.glob(str(os.path.join(FD_folder,\"*.xlsx\"))):\n",
    "    print(FD_file)\n",
    "    FD = pd.read_excel(FD_file)\n",
    "    FD_files.append(FD)\n",
    "FD_all = pd.concat(FD_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember inplace = True\n",
    "FD_all['len_FV'] = FD_all['FV'].str.len()\n",
    "FD_all['len_platform'] = FD_all['Platform'].str.len()\n",
    "FD_all = FD_all.sort_values( ['len_FV','len_platform'] , ascending = [False,False] )\n",
    "FD_all.reset_index( drop = True, inplace = True )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move row with max len to the top row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index where B is longest\n",
    "FD_idmax = FD_all['len_platform'].max()\n",
    "df_max = FD_all.loc[FD_all['len_platform'] == FD_idmax]\n",
    "df_max_to_add_FD = df_max.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isin用在list\n",
    "# drop the max len row\n",
    "FD_all = FD_all.drop([FD_all.index[df_max_to_add_FD.index.values[0]]]).reset_index( drop = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat and put the max len row on the top\n",
    "FD_concat = [df_max_to_add_FD,FD_all]\n",
    "FD_output = pd.concat(FD_concat)\n",
    "FD_output.reset_index( drop = True , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut more than 500\n",
    "FD_output['Platform'] = FD_output['Platform'].apply(lambda x: x[:450] if len(x) > 500 else x)\n",
    "FD_output = FD_output.drop( columns = ['len_FV','len_platform'])\n",
    "FD_output['Item'] = FD_output['Item'].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and clean Shortage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat shortage\n",
    "shortage_files = []\n",
    "for shortage_file in glob.glob(str(os.path.join(shortage_folder,\"*.xlsx\"))):\n",
    "    print(shortage_file)\n",
    "    shortage = pd.read_excel(shortage_file)\n",
    "    shortage_files.append(shortage)\n",
    "shortage_all = pd.concat(shortage_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create value to sort \n",
    "shortage_all['len_FV'] = shortage_all['FV'].str.len()\n",
    "shortage_all['len_platform'] = shortage_all['Platform'].str.len()\n",
    "shortage_all = shortage_all.sort_values(['len_FV','len_platform'] , ascending=[False,False])\n",
    "shortage_all.reset_index( drop = True, inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shortage_all['HP_PN'] = shortage_all['HP_PN'].apply(lambda x: x[:128] if len(x) > 128 else x)\n",
    "except:    \n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move row with max len to the top row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max length \n",
    "shortage_idmax = shortage_all['len_platform'].max()\n",
    "shortage_max = shortage_all.loc[shortage_all['len_platform'] == shortage_idmax]\n",
    "df_max_to_add_shortage = shortage_max.head(1)\n",
    "df_max_to_add_shortage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the max len row\n",
    "shortage_all = shortage_all.drop([shortage_all.index[df_max_to_add_shortage.index.values[0]]]).reset_index( drop = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat and put the max len row on the top\n",
    "shortage_concat = [ df_max_to_add_shortage , shortage_all ]\n",
    "shortage_output = pd.concat(shortage_concat)\n",
    "shortage_output.reset_index( drop = True , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut more than 500\n",
    "shortage_output['Platform'] = shortage_output['Platform'].apply(lambda x: x[:450] if len(x) > 500 else x)\n",
    "shortage_output = shortage_output.drop( columns= ['len_FV','len_platform'])\n",
    "shortage_output['Item'] = shortage_output['Item'].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and clean PNbasedDetail"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat PNbasedDetail\n",
    "PNbasedDetail_files = []\n",
    "for PNbasedDetail_file in glob.glob(str(os.path.join(PNbasedDetail_folder,\"*.xlsx\"))):\n",
    "    print(PNbasedDetail_file)\n",
    "    PNbasedDetail = pd.read_excel(PNbasedDetail_file)\n",
    "    PNbasedDetail_files.append(PNbasedDetail)\n",
    "PNbasedDetail_all = pd.concat(PNbasedDetail_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list to sort \n",
    "character_limit_list = ['GPS Remark','ODM use column1','ODM use column2','ODM use column3','ODM use column4','ODM use column5']\n",
    "sort_list = []\n",
    "asc_list = []\n",
    "for _ in character_limit_list:\n",
    "\n",
    "    try:\n",
    "        PNbasedDetail_all[str('len_' + _)] = PNbasedDetail_all[_].str.len()\n",
    "        sort_list.append(str('len_' + _))\n",
    "        asc_list.append(True)\n",
    "    except:    \n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move row with max len to the top row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort value\n",
    "PNbasedDetail_all = PNbasedDetail_all.sort_values( sort_list , ascending = asc_list )\n",
    "PNbasedDetail_all.reset_index( drop=True , inplace=True ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max length & concat\n",
    "PNbasedDetail_max_files = []\n",
    "\n",
    "# a for loop to calculate all the len max\n",
    "for i in range( 1 , len(sort_list) ):\n",
    "    PNbasedDetail_idmax = PNbasedDetail_all [ sort_list[i] ].max()\n",
    "    PNbasedDetail_max = PNbasedDetail_all.loc[ PNbasedDetail_all[ sort_list[i] ] == PNbasedDetail_idmax ]\n",
    "    PNbasedDetail_max_files.append( PNbasedDetail_max.head(1) )\n",
    "    df_max_to_add_PNbasedDetail_temp = pd.concat( PNbasedDetail_max_files )\n",
    "\n",
    "# sometimes with duplicates \n",
    "df_max_to_add_PNbasedDetail = df_max_to_add_PNbasedDetail_temp.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check point\n",
    "df_max_to_add_PNbasedDetail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the max len row\n",
    "for i in range( 0, len(df_max_to_add_PNbasedDetail.index.values)):\n",
    "    PNbasedDetail_all = PNbasedDetail_all.drop([PNbasedDetail_all.index[df_max_to_add_PNbasedDetail.index.values[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat and put on the top\n",
    "PNbasedDetail_concat_list = [ df_max_to_add_PNbasedDetail , PNbasedDetail_all ]\n",
    "PNbasedDetail_output = pd.concat(PNbasedDetail_concat_list).reset_index( drop = True )\n",
    "PNbasedDetail_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut more than 500\n",
    "for _ in character_limit_list:\n",
    "    try:\n",
    "        PNbasedDetail_output[_] = PNbasedDetail_output[_].apply(lambda x: x[:450] if len(x) > 500 else x)\n",
    "    except:    \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final step, drop calculate step and output\n",
    "PNbasedDetail_output = PNbasedDetail_output.drop( columns = sort_list )\n",
    "PNbasedDetail_output['Item'] = PNbasedDetail_output['Item'].astype(str)\n",
    "PNbasedDetail_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output concated FD, Shortage, and PNbasedDetail files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apache airflow to upload SQL ( currently to desktop )\n",
    "FD_output.to_excel(os.path.join(home, 'Desktop', 'FD_all.xlsx'), index=False)\n",
    "shortage_output.to_excel(os.path.join(home, 'Desktop', 'Shortage_all.xlsx'), index=False)\n",
    "PNbasedDetail_output.to_excel(os.path.join(home, 'Desktop', 'PNbasedDetail_all.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move file to archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FD_folder = os.path.join(targetFolder, \"FD_today\")\n",
    "FD_archive_folder = os.path.join(targetFolder, 'FD_Archive_After_1025')\n",
    "\n",
    "for f in os.listdir(FD_folder):\n",
    "    shutil.move(os.path.join(FD_folder, f), os.path.join(FD_archive_folder, f))\n",
    "    \n",
    "shortage_folder = os.path.join(targetFolder ,\"shortage_today\")\n",
    "shortage_archive_folder = os.path.join(targetFolder ,\"Shortage_Archive_After_1025\")\n",
    "\n",
    "for f in os.listdir(shortage_folder):\n",
    "    shutil.move(os.path.join(shortage_folder, f), os.path.join(shortage_archive_folder, f))\n",
    "\n",
    "PNbasedDetail_folder = os.path.join(targetFolder ,\"PNbasedDetail_today\")\n",
    "PNbasedDetail_archive_folder = os.path.join(targetFolder ,\"PNbasedDetail_Archive_After_1025\")\n",
    "\n",
    "for f in os.listdir(PNbasedDetail_folder):\n",
    "    shutil.move(os.path.join(PNbasedDetail_folder, f), os.path.join(PNbasedDetail_archive_folder, f))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aff61a50ca05787580f84e419a783e3027ed4326834427d692766cd5b4e8a2a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
