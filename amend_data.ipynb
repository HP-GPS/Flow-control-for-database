{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with re-upload or patching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pyodbc\n",
    "import shutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home and time\n",
    "home = Path.home()\n",
    "todaystr = date.today().strftime('%Y-%m-%d')\n",
    "# targetFolder = os.path.join(home, 'HP Inc','GPSTW SOP - 2021 日新','Project team','Upload folder ( for buyer update )')\n",
    "targetFolder = os.path.join(home, 'HP Inc','GPS TW Innovation - Documents','Project team','Project RiXin - Shortage management', 'Upload_folder')\n",
    "\n",
    "# file path\n",
    "FD_path =  os.path.join(Path(home, targetFolder, 'FD_today','amend'))\n",
    "Shortage_path =  os.path.join(Path(home, targetFolder, 'Shortage_today','amend'))\n",
    "PNbasedDetail_path =  os.path.join(Path(home, targetFolder, 'PNbasedDetail_today','amend'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect SQL server\n",
    "conn = pyodbc.connect('Driver={SQL Server Native Client 11.0}; Server=g7w11206g.inc.hpicorp.net; Database=CSI; Trusted_Connection=Yes;')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important maxlen function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxLen(df_all: pd.DataFrame, sort_index: list) -> pd.DataFrame:\n",
    "    # sort based on len\n",
    "    sort_list = []\n",
    "    for _ in sort_index:\n",
    "        try:\n",
    "            df_all[str('len_' + _)] = df_all[_].str.len()\n",
    "            sort_list.append(str('len_' + _))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    df_all = df_all.reset_index(drop = True)\n",
    "\n",
    "    max_files = []\n",
    "    for i, ele in enumerate(sort_list):\n",
    "        idmax = df_all[ele].max()\n",
    "        max = df_all[df_all[ele] == idmax]\n",
    "        max_files.append(max.head(1))\n",
    "    df_max_to_add = pd.concat(max_files).drop_duplicates()\n",
    "\n",
    "    df_max_to_add.index.values.sort()\n",
    "\n",
    "    # drop the max len row\n",
    "    for i, ele in enumerate(df_max_to_add.index.values):\n",
    "        df_all = df_all.drop([df_all.index[ele - i]])\n",
    "\n",
    "    # concat and put on the top\n",
    "    output = pd.concat([df_max_to_add, df_all]).reset_index( drop = True )\n",
    "\n",
    "    # cut more than 500\n",
    "    for _ in sort_index:\n",
    "        try:\n",
    "            output[_] = output[_].apply(lambda x: x[:450] if len(x) > 500 else x)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    # final step, drop calculate step and output\n",
    "    output = output.drop(columns = sort_list)\n",
    "    output['Item'] = output['Item'].astype(str)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete old FD and upload new FD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of all files in the folder with .xlsx extension\n",
    "file_names = [file_name for file_name in os.listdir(FD_path) if file_name.endswith('.xlsx')]\n",
    "\n",
    "for file in file_names:\n",
    "    condition = file.split('_')\n",
    "    print(condition)\n",
    "\n",
    "    # date condition\n",
    "    date_condition = datetime.datetime(int(condition[0][:4]),int(condition[0][4:6]),int(condition[0][6:]))\n",
    "    # commodity condition\n",
    "    commodity_condition = (condition[1])\n",
    "    # ODM condition\n",
    "    ODM_condition = (condition[2])\n",
    "    # buyer condition\n",
    "    buyer_condition = (condition[3])\n",
    "\n",
    "    # delete statement\n",
    "    delete_query = \"DELETE FROM CSI.OPS.GPS_tbl_ops_fd WHERE ReportDate = ? AND Commodity = ? AND ODM = ? AND BuyerName = ?\"\n",
    "    params = (date_condition, commodity_condition, ODM_condition, buyer_condition)\n",
    "    cursor.execute(delete_query, params)\n",
    "\n",
    "    # check result\n",
    "    print(f\"{cursor.rowcount} rows deleted from CSI.OPS.GPS_tbl_ops_fd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hsudu\\Documents\\GitHub\\Flow-control-for-database\\amend_data.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hsudu/Documents/GitHub/Flow-control-for-database/amend_data.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     fd_amend_temp\u001b[39m.\u001b[39mappend(dff)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsudu/Documents/GitHub/Flow-control-for-database/amend_data.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Concatenate all the dataframes into a single dataframe\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hsudu/Documents/GitHub/Flow-control-for-database/amend_data.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m FD_amend_data_temp \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(fd_amend_temp, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsudu/Documents/GitHub/Flow-control-for-database/amend_data.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Max len check\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hsudu/Documents/GitHub/Flow-control-for-database/amend_data.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hsudu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39melif\u001b[39;00m copy \u001b[39mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    370\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    375\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    376\u001b[0m     join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    377\u001b[0m     keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    378\u001b[0m     levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    379\u001b[0m     names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    380\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    381\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    382\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    385\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\hsudu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    426\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[0;32m    428\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the fd\n",
    "fd_amend_temp = []\n",
    "\n",
    "# Loop through all the Excel files in the folder\n",
    "for fd in glob.glob(os.path.join(FD_path, '*.xlsx')):\n",
    "    # Read the data from the Excel file into a pandas dataframe\n",
    "    dff = pd.read_excel(fd)\n",
    "    # Append the dataframe to the list\n",
    "    fd_amend_temp.append(dff)\n",
    "\n",
    "# Concatenate all the dataframes into a single dataframe\n",
    "FD_amend_data_temp = pd.concat(fd_amend_temp, ignore_index=True)\n",
    "\n",
    "# Max len check\n",
    "try:\n",
    "    FD_amend_data = maxLen(FD_amend_data_temp, ['FV','Platform'])\n",
    "except:\n",
    "    FD_amend_data = FD_amend_data_temp.copy()\n",
    "\n",
    "for index, row in FD_amend_data.iterrows():\n",
    "    f_ODM = row['ODM']\n",
    "    f_Item = row['Item']\n",
    "    f_Commodity = row['Commodity']\n",
    "    f_FV = row['FV']\n",
    "    f_Platform = row['Platform']\n",
    "    f_HP_PN = row['HP_PN']\n",
    "    f_Supplier = row['Supplier']\n",
    "    f_PN = row['HP PN']\n",
    "    f_ReportDate = row['ReportDate']\n",
    "    f_FDdate = row['FDdate']\n",
    "    f_FDQty = row['FDQty']\n",
    "    f_BuyerName = row['BuyerName']\n",
    "    \n",
    "    cursor.execute(f\"INSERT INTO CSI.OPS.GPS_tbl_ops_fd ( ODM,Item,Commodity,FV,Platform,Supplier,[HP PN],FDdate,FDQty,Reportdate,BuyerName )\\\n",
    "                    VALUES('{f_ODM}','{f_Item}','{f_Commodity}','{f_FV}','{f_Platform}','{f_Supplier}','{f_PN}','{f_FDdate}','{f_FDQty}','{f_ReportDate}','{f_BuyerName}')\".replace(\"'NaT'\", \"NULL\"))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete old shortage and upload new shortage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20230717', 'GPU', 'CQQCI', 'Jennifer', 'Shortage.xlsx']\n",
      "43 rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\n"
     ]
    }
   ],
   "source": [
    "# Get the names of all files in the folder with .xlsx extension\n",
    "file_names = [file_name for file_name in os.listdir(Shortage_path) if file_name.endswith('.xlsx')]\n",
    "\n",
    "for file in file_names:\n",
    "    condition = file.split('_')\n",
    "    print(condition)\n",
    "\n",
    "    # date condition\n",
    "    date_condition = datetime.datetime(int(condition[0][:4]),int(condition[0][4:6]),int(condition[0][6:]))\n",
    "    # commodity condition\n",
    "    commodity_condition = (condition[1])\n",
    "    # ODM condition\n",
    "    ODM_condition = (condition[2])\n",
    "    # buyer condition\n",
    "    buyer_condition = (condition[3])\n",
    "\n",
    "    # delete statement\n",
    "    delete_query = \"DELETE FROM CSI.OPS.GPS_tbl_ops_shortage_ext WHERE ReportDate = ? AND Commodity = ? AND ODM = ? AND BuyerName = ?\"\n",
    "    params = (date_condition, commodity_condition, ODM_condition, buyer_condition)\n",
    "    cursor.execute(delete_query, params)\n",
    "\n",
    "    # check result\n",
    "    print(f\"{cursor.rowcount} rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the Shortage\n",
    "Shortage_amend_temp = []\n",
    "\n",
    "# Loop through all the Excel files in the folder\n",
    "for Shortage in glob.glob(os.path.join(Shortage_path, '*.xlsx')):\n",
    "    # Read the data from the Excel file into a pandas dataframe\n",
    "    dfs = pd.read_excel(Shortage)\n",
    "    # Append the dataframe to the list\n",
    "    Shortage_amend_temp.append(dfs)\n",
    "\n",
    "# Concatenate all the dataframes into a single dataframe\n",
    "Shortage_amend_data_temp = pd.concat(Shortage_amend_temp, ignore_index=True)\n",
    "\n",
    "# Max len check\n",
    "try:\n",
    "    Shortage_amend_data_temp['HP_PN'] = Shortage_amend_data_temp['HP_PN'].apply(lambda x: x[:128] if len(x) > 128 else x)\n",
    "except:    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    Shortage_amend_data = maxLen(Shortage_amend_data_temp, ['Platform','FV'])\n",
    "except:\n",
    "    Shortage_amend_data = Shortage_amend_data_temp.copy()\n",
    "\n",
    "for index, row in Shortage_amend_data.iterrows():\n",
    "    s_ODM = row['ODM']\n",
    "    s_Item = row['Item']\n",
    "    s_Commodity = row['Commodity']\n",
    "    s_FV = row['FV']\n",
    "    s_Platform = row['Platform']\n",
    "    s_P1 = row['P1']\n",
    "    s_P2 = row['Net P2']\n",
    "    s_P3 = row['Net P3']\n",
    "    s_Total = row['Total Shortage Qty']\n",
    "    s_BT = row['BT shortage']\n",
    "    s_working = row['Working on upside']\n",
    "    s_ReportDate = pd.to_datetime(row['ReportDate'])\n",
    "    s_lastFDdate = pd.to_datetime(row['last FD date'])\n",
    "    s_BuyerName = row['BuyerName']\n",
    "    s_PN = row['HP_PN']\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO CSI.OPS.GPS_tbl_ops_shortage_ext ( ODM,Item,Commodity,FV,Platform,P1,[Net P2],[Net P3],[Total Shortage Qty],[BT shortage],[Working on upside],ReportDate,[last FD date],HP_PN,BuyerName )\\\n",
    "                    VALUES('{s_ODM}','{s_Item}','{s_Commodity}','{s_FV}','{s_Platform}','{s_P1}','{s_P2}','{s_P3}','{s_Total}','{s_BT}','{s_working}','{s_ReportDate}','{s_lastFDdate}','{s_PN}','{s_BuyerName}')\".replace(\"'NaT'\", \"NULL\"))\n",
    "\n",
    "conn.commit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete old PNbasedDetail and upload new PNbasedDetail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20230717', 'GPU', 'CQQCI', 'Jennifer', 'PNbasedDetail.xlsx']\n",
      "30 rows deleted from CSI.OPS.GPS_tbl_ops_PNbasedDetail\n"
     ]
    }
   ],
   "source": [
    "# Get the names of all files in the folder with .xlsx extension\n",
    "file_names = [file_name for file_name in os.listdir(PNbasedDetail_path) if file_name.endswith('.xlsx')]\n",
    "\n",
    "for file in file_names:\n",
    "    condition = file.split('_')\n",
    "    print(condition)\n",
    "\n",
    "    # date condition\n",
    "    date_condition = datetime.datetime(int(condition[0][:4]),int(condition[0][4:6]),int(condition[0][6:]))\n",
    "    # commodity condition\n",
    "    commodity_condition = (condition[1])\n",
    "    # ODM condition\n",
    "    ODM_condition = (condition[2])\n",
    "    # buyer condition\n",
    "    buyer_condition = (condition[3])\n",
    "\n",
    "    # delete statement\n",
    "    delete_query = \"DELETE FROM CSI.OPS.GPS_tbl_ops_PNbasedDetail WHERE ReportDate = ? AND Commodity = ? AND ODM = ? AND BuyerName = ?\"\n",
    "    params = (date_condition, commodity_condition, ODM_condition, buyer_condition)\n",
    "    cursor.execute(delete_query, params)\n",
    "\n",
    "    # check result\n",
    "    print(f\"{cursor.rowcount} rows deleted from CSI.OPS.GPS_tbl_ops_PNbasedDetail\" )\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the PNbasedDetail\n",
    "PNbasedDetail_amend_temp = []\n",
    "\n",
    "# Loop through all the Excel files in the folder\n",
    "for PNbasedDetail in glob.glob(os.path.join(PNbasedDetail_path, '*.xlsx')):\n",
    "    # Read the data from the Excel file into a pandas dataframe\n",
    "    dfp = pd.read_excel(PNbasedDetail)\n",
    "    # Append the dataframe to the list\n",
    "    PNbasedDetail_amend_temp.append(dfp)\n",
    "\n",
    "# Concatenate all the dataframes into a single dataframe\n",
    "PNbasedDetail_amend_data_temp = pd.concat(PNbasedDetail_amend_temp, ignore_index=True)\n",
    "\n",
    "# fillna for PNbasedDetail\n",
    "for i in ['GPS Remark', 'ODM use column1','ODM use column2','ODM use column3','ODM use column4','ODM use column5']:\n",
    "    PNbasedDetail_amend_data_temp[i] = PNbasedDetail_amend_data_temp[i].fillna(\"\")\n",
    "\n",
    "# Max len check\n",
    "try:\n",
    "    PNbasedDetail_amend_data = maxLen(PNbasedDetail_amend_data_temp, ['GPS Remark','ODM use column1','ODM use column2','ODM use column3','ODM use column4','ODM use column5'])\n",
    "except:\n",
    "    PNbasedDetail_amend_data = PNbasedDetail_amend_data_temp.copy()\n",
    "\n",
    "\n",
    "for index, row in PNbasedDetail_amend_data.iterrows():\n",
    "    p_ODM = row['ODM']\n",
    "    p_Item = row['Item']\n",
    "    p_Commodity = row['Commodity']\n",
    "    p_PN = row['HP PN']\n",
    "    p_Remark = str(row['GPS Remark']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_stock = row['852 stock']\n",
    "    p_change = row['852 stock change']\n",
    "    p_over = row['Over pull qty']\n",
    "    p_ODM1 = str(row['ODM use column1']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_ODM2 = str(row['ODM use column2']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_ODM3 = str(row['ODM use column3']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_ODM4 = str(row['ODM use column4']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_ODM5 = str(row['ODM use column5']).replace(\"\\'\", \"\\'\\'\")\n",
    "    p_ReportDate = row['ReportDate']\n",
    "    p_BuyerName = row['BuyerName']\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO CSI.OPS.GPS_tbl_ops_PNbasedDetail ( ODM,Item,Commodity,[HP PN],[GPS Remark],[852 stock],[852 stock change],[Over pull qty],\\\n",
    "                    [ODM use column1],[ODM use column2],[ODM use column3],[ODM use column4],[ODM use column5],ReportDate,BuyerName )\\\n",
    "                    VALUES('{p_ODM}','{p_Item}','{p_Commodity}','{p_PN}','{p_Remark}','{p_stock}','{p_change}','{p_over}','{p_ODM1}','{p_ODM2}','{p_ODM3}','{p_ODM4}','{p_ODM5}','{p_ReportDate}','{p_BuyerName}')\".replace(\"'NaT'\", \"NULL\"))\n",
    "\n",
    "conn.commit()\n",
    "# conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move uploaded data to archieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FD Archieve folder define\n",
    "FD_archive_folder = os.path.join(targetFolder, 'FD_Archive')\n",
    "# Move FD\n",
    "for f in os.listdir(FD_path):\n",
    "    if f.endswith('.xlsx'):\n",
    "        shutil.move(os.path.join(FD_path, f), os.path.join(FD_archive_folder, f))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# Shortage Archieve folder define\n",
    "shortage_archive_folder = os.path.join(targetFolder ,\"Shortage_Archive\")\n",
    "# Move Shortage\n",
    "for f in os.listdir(Shortage_path):\n",
    "    if f.endswith('.xlsx'):\n",
    "        shutil.move(os.path.join(Shortage_path, f), os.path.join(shortage_archive_folder, f))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# PNbasedDetail Archieve folder define\n",
    "PNbasedDetail_archive_folder = os.path.join(targetFolder ,\"PNbasedDetail_Archive\")\n",
    "# Move PNbasedDetail\n",
    "for f in os.listdir(PNbasedDetail_path):\n",
    "    if f.endswith('.xlsx'):\n",
    "        shutil.move(os.path.join(PNbasedDetail_path, f), os.path.join(PNbasedDetail_archive_folder, f))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "PNbasedDetail_archive_folder = os.path.join(targetFolder ,\"PNbasedDetail_Archive\")\n",
    "# Move PNbasedDetail\n",
    "for f in os.listdir(PNbasedDetail_path):\n",
    "    if f.endswith('.xlsx'):\n",
    "        shutil.move(os.path.join(PNbasedDetail_path, f), os.path.join(PNbasedDetail_archive_folder, f))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add process to remove duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\" SELECT * FROM CSI.OPS.GPS_tbl_ops_shortage_ext\")\n",
    "# find the rows that are not on database\n",
    "allshortage = pd.DataFrame.from_records(cursor.fetchall(), columns = [i[0] for i in cursor.description])\n",
    "\n",
    "# extract duplicated rows\n",
    "duplicate_rows = allshortage[allshortage.duplicated()]\n",
    "# delete them all in database\n",
    "for index,row in duplicate_rows.iterrows():\n",
    "    delete_query = \"DELETE FROM CSI.OPS.GPS_tbl_ops_shortage_ext WHERE ReportDate =? AND ODM=? AND Item=? AND Commodity=? AND FV=? AND Platform=? AND BuyerName=? \"\n",
    "    cursor.execute(delete_query,(row['ReportDate'],row['ODM'],row['Item'],row['Commodity'],row['FV'],row['Platform'],row['BuyerName']))\n",
    "    if cursor.rowcount:\n",
    "        # print(row)\n",
    "        print(f\"{cursor.rowcount} rows deleted from CSI.OPS.GPS_tbl_ops_shortage_ext\")\n",
    "conn.commit()\n",
    "# upload them in database\n",
    "for index, row in duplicate_rows.iterrows():\n",
    "    s_ODM = row['ODM']\n",
    "    s_Item = row['Item']\n",
    "    s_Commodity = row['Commodity']\n",
    "    s_FV = row['FV']\n",
    "    s_Platform = row['Platform']\n",
    "    s_P1 = row['P1']\n",
    "    s_P2 = row['Net P2']\n",
    "    s_P3 = row['Net P3']\n",
    "    s_Total = row['Total Shortage Qty']\n",
    "    s_BT = row['BT shortage']\n",
    "    s_working = row['Working on upside']\n",
    "    s_ReportDate = pd.to_datetime(row['ReportDate'])\n",
    "    s_lastFDdate = pd.to_datetime(row['last FD date'])\n",
    "    try:\n",
    "        s_BuyerName = row['BuyerName'].replace('\\)','')\n",
    "    except:\n",
    "        s_BuyerName = row['BuyerName']\n",
    "    s_PN = row['HP_PN']\n",
    "\n",
    "    cursor.execute(f\"INSERT INTO CSI.OPS.GPS_tbl_ops_shortage_ext ( ODM,Item,Commodity,FV,Platform,P1,[Net P2],[Net P3],[Total Shortage Qty],[BT shortage],[Working on upside],ReportDate,[last FD date],HP_PN,BuyerName )\\\n",
    "                    VALUES('{s_ODM}','{s_Item}','{s_Commodity}','{s_FV}','{s_Platform}','{s_P1}','{s_P2}','{s_P3}','{s_Total}','{s_BT}','{s_working}','{s_ReportDate}','{s_lastFDdate}','{s_PN}','{s_BuyerName}')\".replace(\"'NaT'\", \"NULL\"))\n",
    "\n",
    "\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cadcbe1cf7f77607d04fb86883766795fe82998168b094e073d163801885097d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
